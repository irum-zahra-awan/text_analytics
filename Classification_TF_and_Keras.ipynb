{"cells":[{"cell_type":"code","source":["dbutils.library.installPyPI(\"fuzzywuzzy\")\n# dbutils.library.installPyPI(\"Keras\")\n# dbutils.library.installPyPI(\"nltk\")\n# dbutils.library.installPyPI(\"numpy\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ba2b757-6f0b-4443-871e-a7fe328ac6da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["! /databricks/python/bin/pip install --upgrade pip\n! /databricks/python/bin/pip install nltk\n! /databricks/python/bin/python -m nltk.downloader stopwords\n! /databricks/python/bin/python -m nltk.downloader punkt\n! /databricks/python/bin/python -m nltk.downloader wordnet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b879d9-45a1-467b-a095-a42952c6eb07"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: nltk in /databricks/python3/lib/python3.7/site-packages (3.5)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.7/site-packages (from nltk) (4.59.0)\r\nRequirement already satisfied: regex in /databricks/python3/lib/python3.7/site-packages (from nltk) (2020.11.13)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.7/site-packages (from nltk) (7.1.2)\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package stopwords to /home/root/nltk_data...\r\n[nltk_data]   Package stopwords is already up-to-date!\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package punkt to /home/root/nltk_data...\r\n[nltk_data]   Package punkt is already up-to-date!\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package wordnet to /home/root/nltk_data...\r\n[nltk_data]   Package wordnet is already up-to-date!\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: nltk in /databricks/python3/lib/python3.7/site-packages (3.5)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.7/site-packages (from nltk) (4.59.0)\r\nRequirement already satisfied: regex in /databricks/python3/lib/python3.7/site-packages (from nltk) (2020.11.13)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.7/site-packages (from nltk) (7.1.2)\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package stopwords to /home/root/nltk_data...\r\n[nltk_data]   Package stopwords is already up-to-date!\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package punkt to /home/root/nltk_data...\r\n[nltk_data]   Package punkt is already up-to-date!\r\n/usr/lib/python3.7/runpy.py:125: RuntimeWarning: &#39;nltk.downloader&#39; found in sys.modules after import of package &#39;nltk&#39;, but prior to execution of &#39;nltk.downloader&#39;; this may result in unpredictable behaviour\r\n  warn(RuntimeWarning(msg))\r\n[nltk_data] Downloading package wordnet to /home/root/nltk_data...\r\n[nltk_data]   Package wordnet is already up-to-date!\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["! /databricks/python/bin/pip install keras\n! /databricks/python/bin/pip install tensorflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69ed2909-2d0c-49ce-a730-5be7f43a14b8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from fuzzywuzzy import fuzz \nfrom fuzzywuzzy import process \nimport nltk\nfrom pyspark.sql.functions import *\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport re\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0605d3d0-fbfe-4ff4-b224-7eb3386f2c52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def chat_summerizer(text):\n  # Tokenizing the text \n  stopWords = set(stopwords.words(\"english\")) \n  words = word_tokenize(text)\n  sentences = sent_tokenize(text) \n  \n  # Creating a frequency table to keep the score of each word \n  freqTable = dict() \n  for word in words: \n      word = word.lower() \n      if word in stopWords: \n          continue\n      if word in freqTable: \n          freqTable[word] += 1\n      else: \n          freqTable[word] = 1\n\n  # Creating a dictionary to keep the score of each sentence \n  sentences = sent_tokenize(text) \n  sentenceValue = dict() \n   \n  for sentence in sentences: \n      for word, freq in freqTable.items(): \n          if word in sentence.lower(): \n              if sentence in sentenceValue: \n                  sentenceValue[sentence] += freq \n              else: \n                  sentenceValue[sentence] = freq \n                  \n  sumValues = 0\n  for sentence in sentenceValue: \n      sumValues += sentenceValue[sentence] \n      \n  # Average value of a sentence from the original text \n  average = int(sumValues / len(sentenceValue)) \n  \n  # Storing sentences into our summary\n  summary = '' \n  for sentence in sentences: \n      if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): \n          summary += \" \" + sentence \n  # print(summary)\n  return summary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"chat_summerizer","showTitle":true,"inputWidgets":{},"nuid":"651d0d27-c829-450a-b697-5c7a854ad2ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def clean_text(text):\n    \"\"\"\n    Applies some pre-processing on the given text.\n\n    Steps :\n    - Removing HTML tags\n    - Removing punctuation\n    - Lowering text\n    \"\"\"\n    # remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    \n    # remove the characters [\\], ['] and [\"]\n    text = re.sub(r\"\\\\\", \"\", text)    \n    text = re.sub(r\"\\'\", \"\", text)    \n    text = re.sub(r\"\\\"\", \"\", text)    \n    \n    # convert text to lowercase\n    text = text.strip().lower()\n    \n    # replace punctuation characters with spaces\n    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n\n    return text\n  \ndef lemmatize_text(text):\n    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    return [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n\ndef pre_processing(df,col_name):\n  df['cleaned_text'] = df[col_name].apply(lambda x: (clean_text(x)))\n  df['cleaned_text'] = df['cleaned_text'].str.replace('[^\\w\\s]','')\n  stop = stopwords.words('english')\n  df['stopwords_removed'] = df['cleaned_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n  df['text_lemmatized'] = df.stopwords_removed.apply(lemmatize_text)\n  df['text_lemmatized']=df['text_lemmatized'].apply(lambda x: \" \".join(a for a in x))\n  # df['text_lemmatized'] = df['text_lemmatized'].str.replace('[^\\w\\s]','')\n  return df\n\ndef prep_train_data():\n  df_types = spark.table(\"irumdb.cs_types_list\")\n  df_types=df_types.filter(df_types.Questions.isNotNull())\n  df_types=df_types.na.replace(['14 Day trial '], [None], 'Questions')\n  df_types=df_types.filter(df_types.Questions.isNotNull())\n  \n  df_types_pd = df_types.toPandas()\n  final_df = pre_processing(df_types_pd,col_name='Questions')\n  return final_df\n  \ndef prep_chat_data():\n  df_chats= spark.table(\"live_admin_chat.chatmessages\")\n  df_chats_pd=df_chats.toPandas()\n  \n  # TAKING ONLY FIRST 100 CHATS FOR CHECKING THE CONCEPT\n  new_df=df_chats_pd[[\"ChatId\",\"MessageText\",\"MessageBy\"]][100:200].groupby([\"ChatId\",\"MessageBy\"],as_index=False).aggregate(lambda x: list(x))\n  new_df['messages_split']=new_df['MessageText'].apply(lambda x: \" \".join(a for a in x))\n  new_df=new_df[~(new_df.messages_split.str.contains('Hi, welcome to the ENTERTAINER Live Chat'))]\n  new_df=new_df[~(new_df.messages_split.str.contains('Your patience is much appreciated'))]\n  new_df=new_df[~(new_df.messages_split.str.contains('NAVIGATEURL'))]\n  new_df.reset_index(drop=True,inplace = True)\n  \n  new_df_sp = spark.createDataFrame(new_df)\n  new_df_sp = new_df_sp.select('ChatId','messages_split').groupby('ChatId').agg(collect_set('messages_split')).sort(['ChatId'],ascending=True)\n  \n  new_df_pd=new_df_sp.toPandas()\n  new_df_pd.rename(columns={\"collect_set(messages_split)\":\"list_of_chats\"},inplace=True)\n  new_df_pd['list_of_chats']=new_df_pd['list_of_chats'].apply(lambda x: \" \".join(a for a in x))\n  \n  new_df_pd['lower_chats'] = new_df_pd['list_of_chats'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n  # FIRST TRY  \n  # TEXT SUMMERIZER\n  new_df_pd['summarized_text'] = new_df_pd['lower_chats'].apply(lambda x: (chat_summerizer(x)))\n  \n  stop = stopwords.words('english')\n  # new_df_pd['stopwords_removed'] = new_df_pd['lower_chats'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n  new_df_pd['stopwords_removed'] = new_df_pd['summarized_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n  new_df_pd['stopwords_removed'] = new_df_pd['stopwords_removed'].str.replace('[^\\w\\s]','')\n  freq=['nickchange','u','ok','okay','hi','thx','hello','entertainer','thanks','thankyou','thank you']\n  new_df_pd['most_freq_removed'] = new_df_pd['stopwords_removed'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n  \n  df_chats=pre_processing(new_df_pd,col_name='most_freq_removed')\n  \n#   # SECOND TRY TOTAL FLOP\n#   # TEXT SUMMERIZER\n#   df_chats['summarized_text'] = df_chats['most_freq_removed'].apply(lambda x: (chat_summerizer(x)))\n  \n  # df_chats=spark.createDataFrame(new_df_pd)\n  # df_chats=df_chats.select('ChatId','most_freq_removed')\n  return df_chats\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Prep Data","showTitle":true,"inputWidgets":{},"nuid":"680ac7d8-9760-4325-b97c-5f1c818bfa10"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_quest = prep_train_data()\ndf_quest['cleaned_text'] = df_quest['cleaned_text'].str.replace('[^\\w\\s]','')\ndf_quest.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8db10ce9-eb8d-41d6-9884-ab2afefbfdf5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_chats = prep_chat_data()\ndf_chats.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9302f6b-5d8f-42a3-80ab-79d4fc38cb69"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Drop any rows with feature col empty\ndf_chats=df_chats.drop(df_chats.index[[2,3,5]])\ndf_chats.reset_index(drop=True,inplace = True)\ndf_chats.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1f971b-f479-4510-9437-cc623d0c997a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# KEEP FIRST ENTRY FROM EACH GROUP FOR TESTING\ndf_test=df_quest.groupby('Categroy').first().reset_index()\ndf_test.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b3cfc39-06df-4a56-89bc-5d2f5a176087"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\ndf_test['labels'] = labelencoder.fit_transform(df_test['Categroy'])\ndf_test.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b4ce864-9428-4ad3-bb0a-5b4700a64f49"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DROP FIRST ENTRY FROM EACH GROUP AND KEEP THE REST FOR TRAINING\ndf_train=df_quest.drop(df_quest.groupby(['Categroy']).head(1).index, axis=0)\ndf_train.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f09225b3-5239-4255-9999-8406088ba94d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\ndf_train['labels'] = labelencoder.fit_transform(df_train['Categroy'])\ndf_train.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db9bf83f-110c-419b-87e7-300da2032f57"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Train & Test subsets\nX_train, y_train = df_train.iloc[:, 2].values, df_train.iloc[:, 0].values.reshape(-1, 1)\nX_test, y_test = df_test.iloc[:, 2].values, df_test.iloc[:, 0].values.reshape(-1, 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31c2365f-e79f-47e4-ad08-272aa3e1fb22"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# OHE the Categorical column\nfrom sklearn.preprocessing import OneHotEncoder as ohe\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(transformers = [('one_hot_encoder', ohe(categories = 'auto'), [0])],\n                       remainder = 'passthrough')\n\ny_train = ct.fit_transform(y_train) #.todense()\ny_test = ct.transform(y_test) #.todense()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f190b71-31dd-454f-ab23-91a27833a5e0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Setting some paramters\nvocab_size = 2000\nsequence_length = 100"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67d6fe9b-e788-4c22-9af5-ff0773bc1153"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Tokenization with Keras\nfrom keras.preprocessing.text import Tokenizer\n\ntk = Tokenizer(num_words = vocab_size)\ntk.fit_on_texts(X_train)\n\nX_train = tk.texts_to_sequences(X_train)\nX_test = tk.texts_to_sequences(X_test)\n\n# Padding all questions with zeros\nfrom keras.preprocessing.sequence import pad_sequences\n\nX_train_seq = pad_sequences(X_train, maxlen = sequence_length, padding = 'post')\nX_test_seq = pad_sequences(X_test, maxlen = sequence_length, padding = 'post')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac25a567-e7f1-4f48-a75b-c4fe0299d53d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Training the Embedding Layer & the Neural Network\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Flatten\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim = vocab_size, output_dim = 5, input_length = sequence_length))\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 3, activation = 'softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = 'rmsprop',\n              metrics = ['accuracy'])\n\nmodel.summary()\n\nhistory = model.fit(X_train_seq, y_train, epochs = 20, batch_size = 512, verbose = 1)\n\n# Save model once done training\n#model.save(\"model.h5\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d449015e-6f65-4c77-a631-0d999aafe131"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Classification_TF_and_Keras","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":216174103078764}},"nbformat":4,"nbformat_minor":0}
